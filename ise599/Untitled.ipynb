{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'preprocessor' has no attribute 'api'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2dc4be3ea9d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# tweepy preprocessing called for basic preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mclean_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;31m# call clean_tweet method for extra preprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[0mfiltered_tweet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'preprocessor' has no attribute 'api'"
     ]
    }
   ],
   "source": [
    "# Emoji patterns\n",
    "import os\n",
    "import pandas as pd\n",
    "import tweepy\n",
    "import re\n",
    "import string\n",
    "# from textblob import TextBlob\n",
    "import preprocessor\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "COLS = ['id', 'created_at', 'source', 'original_text', 'clean_text', 'sentiment', 'polarity', 'subjectivity', 'lang',\n",
    "        'favorite_count', 'retweet_count', 'original_author', 'possibly_sensitive', 'hashtags',\n",
    "        'user_mentions', 'place', 'place_coord_boundaries']\n",
    "\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "# Happy Emoticons\n",
    "emoticons_happy = set([\n",
    "    ':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}',\n",
    "    ':^)', ':-D', ':D', '8-D', '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D',\n",
    "    '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P', ':-P', ':P', 'X-P',\n",
    "    'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)',\n",
    "    '<3'\n",
    "])\n",
    "\n",
    "# Sad Emoticons\n",
    "emoticons_sad = set([\n",
    "    ':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<',\n",
    "    ':-[', ':-<', '=\\\\', '=/', '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c',\n",
    "    ':c', ':{', '>:\\\\', ';('\n",
    "])\n",
    "\n",
    "emoticons = emoticons_happy.union(emoticons_sad)\n",
    "\n",
    "# mrhod clean_tweets()\n",
    "def clean_tweets(tweet):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(tweet)\n",
    "\n",
    "    # remove emojis from tweet\n",
    "    tweet = emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "    # looping through conditions\n",
    "    for w in word_tokens:\n",
    "        # check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n",
    "    # print(word_tokens)\n",
    "    # print(filtered_sentence)\n",
    "    # filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_tweet = []\n",
    "\n",
    "    # looping through conditions\n",
    "    for w in word_tokens:\n",
    "        # check tokens against stop words , emoticons and punctuations\n",
    "        if w not in stop_words and w not in emoticons and w not in string.punctuation:\n",
    "            filtered_tweet.append(w)\n",
    "    return ' '.join(filtered_tweet)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_json('C:/Users/Claire/IdeaProjects/ise599/tweets (1).json', lines=True)\n",
    "\n",
    "new_entry = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "\n",
    "    # tweepy preprocessing called for basic preprocessing\n",
    "    clean_text = preprocessor.api.clean(df['text'][i])\n",
    "    # call clean_tweet method for extra preprocessing\n",
    "    filtered_tweet = clean_tweets(clean_text)\n",
    "\n",
    "    # remove emojis from tweet\n",
    "    \n",
    "\n",
    "    filtered_tweet = clean_text.lower()  # convert lowercase\n",
    "    filtered_tweet = filtered_tweet.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "\n",
    "    if len(filtered_tweet.strip().split()) > 5:  # must have 5 more words\n",
    "        blob = TextBlob(filtered_tweet)\n",
    "        Sentiment = blob.sentiment\n",
    "\n",
    "#         seperate polarity and subjectivity in to two variables\n",
    "        polarity = Sentiment.polarity\n",
    "        subjectivity = Sentiment.subjectivity\n",
    "\n",
    "        new_entry.append([df['created_at'][i],filtered_tweet, Sentiment, polarity, subjectivity,df['label'][i]])\n",
    "\n",
    "\n",
    "newdataset= pd.DataFrame(new_entry, columns=['created_at',\"filtered_tweet\", \"Sentiment\", \"polarity\", \"subjectivity\",'label'])\n",
    "print(newdataset.shape)\n",
    "newdataset.to_csv(r'C:/Users/Claire/IdeaProjects/ise599/cleanedData1023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
